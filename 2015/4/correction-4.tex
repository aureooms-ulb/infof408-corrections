\documentclass{article}
\errorcontextlines 10000

\makeatletter

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}

\usepackage{fullpage}
\usepackage{mleftright}

\usepackage{hyperref}

\hypersetup{%
	linktocpage  = true, %page number is the link (not title)
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = darkblue, %Colour for external hyperlinks
	linkcolor    = darkblue, %Colour of internal links
	citecolor    = darkblue   %Colour of citations
}

%\hypersetup{
%	linktocpage  = true, %page number is the link (not title)
%	colorlinks   = true, %Colours links instead of ugly boxes
%	allcolors = bookColor,
%	hidelinks = true
%}

% good looking urls
\urlstyle{same}

\newcommand{\theoremname}{Theorem}
\newcommand{\problemname}{Problem}
\newcommand{\conjecturename}{Conjecture}
\newcommand{\definitionname}{Definition}

% we use this for our references as well
\let\sref\ref
\AtBeginDocument{\renewcommand{\ref}[1]{\mbox{\autoref{#1}}}}
\usepackage[nameinlink]{cleveref}

% define colors %
\usepackage[table]{xcolor}

%\usepackage[table,dvipdfx,cmyk]{xcolor}
%\definecolor{bookColor}{cmyk}{0 ,0 ,0 ,1}
%\color{bookColor}

\definecolor{lightgray}{rgb}{0.9,0.9,0.9}
\definecolor{darkgray}{rgb}{0.4,0.4,0.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\definecolor{darkblue}{rgb}{0.02, 0.17, 0.40}
\usepackage[stable]{footmisc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{url}
\usepackage{amsmath,amsthm,amsfonts,mathtools}
\usepackage{xfrac}
\newtheorem{theorem}{\theoremname}
\newtheorem{problem}{\problemname}
\newtheorem{conjecture}{\conjecturename}
\newtheorem{definition}{\definitionname}

% nicely spaced operators
\DeclareMathOperator{\Exists}{\exists}
\DeclareMathOperator{\Forall}{\forall}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand{\card}[1]{|#1|}

\newcommand{\st}{\colon\,}
\newcommand{\TM}{TM}
\newcommand{\Atm}{A\textsubscript{TM}}
\newcommand{\HALTtm}{HALT\textsubscript{TM}}

\newcommand\N{\mathbb{N}_1}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}



% TURING MACHINE DESCRIPTIONS
\usepackage{changepage}
\newenvironment{steps}%
{%
\vspace{0.25cm}%
\begin{adjustwidth}{0.3cm}{0cm}%
\begin{description}%
}
{%
\end{description}%
\end{adjustwidth}%
\vspace{0.1cm}%
}

\newcounter{TMachine}[section]
\renewcommand{\theTMachine}{\thesection.\arabic{TMachine}}%
\newenvironment{TMachine}[1]
  {\refstepcounter{TMachine}%
   \par%
   \vspace{.5\baselineskip\@plus.2\baselineskip\@minus.2\baselineskip}% Space above
   \noindent{#1}%
   \begin{steps}}%\begin{TMachine}
  {\end{steps}%
\vspace{.5\baselineskip\@plus.2\baselineskip\@minus.2\baselineskip}% Space below
}% \end{TMachine}

\newcommand{\accept}{\emph{accept}}
\newcommand{\reject}{\emph{reject}}

\makeatother

\title{Computability and Complexity:\\Exercise Session 4 (2015-10-16)}
\author{AurÃ©lien Ooms\footnote{aureooms@ulb.ac.be}}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Exercises 7.1 and 7.2\footnote{Exercises from the reference book: Sipser M.,
\emph{Introduction to the Theory of Computation}, 3rd edition (2013).}}
Those exercises are simple to understand once you have understood the definitions for the
\emph{Big-O} and \emph{Small-O} notations\footnote{We saw these definitions
during last lecture and it is explained at the beginning of Chapter 7.}.

\subsection{Exercise 7.1}
As a reminder
\begin{definition}
	\(f(n) = O(g(n)) \iff \exists c, n_0\) such that \(f(n) \le c g(n)\) for
	all \(n > n_0\).
\end{definition}

\paragraph{(a)}
\(2n = O(n)\)
is true because \(c = 2\) and \(n_0 = 0\) verify the definition.

\paragraph{(b)}
\(n^2 = O(n)\)
is false because for any \(c \ge 1\) we have that for all \(n > c\), \(n^2 > cn\).

\paragraph{(c)}
\(n^2 = O(n \log^2 n)\)
is false because $n$ grows faster than $\log^2 n$ (hint: check the derivatives).

\paragraph{(d)}
\(n \log n = O(n^2)\)
is true because $\log n$ grows slower than $n$ (hint: check the derivatives).
For instance, \(c = 1\) and \(n_0 = 0\) verify the definition.

\paragraph{(e)}
\(3^n = 2^{O(n)}\)
is true because \(c = \log_2 3\) and \(n_0 = 0\) verify the definition.

\paragraph{(f)}
\(2^{2^n} = O(2^{2^n})\)
is true because \(c = 1\) and \(n_0 = 0\) verify the definition. Note that we
always have \(f(n) = O(f(n))\).

\subsection{Exercise 7.2}
As a reminder
\begin{definition}
	\(f(n) = o(g(n)) \iff \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0\).
\end{definition}

\paragraph{(a)}
\(n = o(2n)\) is false because \(\lim_{n \to \infty} \frac{n}{2n} = \lim_{n \to
\infty} \frac{1}{2} = \frac{1}{2} \neq 0\). Note that we always have \(f(n)
\neq o(k f(n))\) for all constant \(k\).

\paragraph{(b)}
\(2n = o(n^2)\) is true because \(\lim_{n \to \infty} \frac{2n}{n^2} =
\lim_{n \to \infty} \frac{2}{n} = 0\)

\paragraph{(c)}
\(2^n = o(3^n)\) is true because \(\lim_{n \to \infty} \frac{2^n}{3^n} =
\lim_{n \to \infty} \mleft(\frac{2}{3}\mright)^n = 0\).

\paragraph{(d)}
\(1 = o(n)\) is true because \(\lim_{n \to \infty} \frac{1}{n} = 0\).

\paragraph{(e)}
\(n = o(\log n)\) is false because \(\lim_{n \to \infty} \frac{n}{\log n} =
\infty \neq 0\).

\paragraph{(f)}
\(1 = o(\sfrac{1}{n})\) is false because \(\lim_{n \to \infty}
\frac{1}{\sfrac{1}{n}} = \lim_{n \to \infty} n = \infty \neq 0\).


\section{Exercise 5.18}
During the last exercise session we showed that there are decidable variants
of Post's correspondence problem. The variants we saw add some constraint on
the input of the problem. For example, if the string alphabet is \(\Sigma =
\{1\}\) or if the top and bottom string of each domino must have the same
length then the problem of finding a match becomes decidable.

To show that the PCP is undecidable, we reduced \Atm{} to PCP, showing that we
could simulate a complete run of a \TM{} by trying to find a match on a
set of dominos. The string alphabet in this case contained a symbol for each
state of the \TM{}, for each symbol of the input tape and also the symbols
\texttt{\(\star\)} and \texttt{\(\Diamond\)}.
What we now show is that an alphabet of size \(2\) is enough to make
the problem undecidable.

Given \(\Gamma = \{\alpha_1, \alpha_2, \ldots, \alpha_n\}\), an alphabet of size \(n\),
we can map any PCP instance over this alphabet to a PCP instance over alphabet
\(\Sigma = \{0,1\}\). We have several options, the next three subsections give
three possible mappings. For each of these mapping we can construct a PCP
instance over \(\Sigma\)
by translating each domino of the instance over \(\Gamma\) to a domino composed
of symbols of \(\Sigma\). To each match of this new instance corresponds one,
and only one, match of the original instance.

\subsection{Using a unary-like encoding}
We map \(\alpha_1\) to \texttt{01}, \(\alpha_2\) to \texttt{011}, etc., that
is, \(\alpha_i\) to
\(f(\alpha_i) =
\texttt{0}\mathrlap{\overbrace{\phantom{\texttt{1}\ldots
\texttt{1}\strut}}^{i~\text{times}}}\texttt{1}\ldots\texttt{1}\).
Since each symbol image \(f(\alpha_i)\) starts with a \texttt{0} and
contains exactly one \texttt{0},
we can only match a symbol that appears at the same position in the top
and in the bottom string of a match in the original PCP instance by matching
the mapped symbol in the top and bottom string of the constructed PCP instance.
In other words, to each match for the original PCP instance corresponds a match
for the constructed PCP instance and vice versa.

\subsection{Using fixed size words}
We map \(\alpha_1\) to
\(\overbrace{\texttt{0}\ldots\texttt{00}}^{%
\scriptsize\begin{tabular}{c}\(\ceil{\log n}\)\\symbols\end{tabular}}\),
\(\alpha_2\) to
\(\overbrace{\texttt{0}\ldots\texttt{01}}^{%
\scriptsize\begin{tabular}{c}\(\ceil{\log n}\)\\symbols\end{tabular}}\),
etc., that is,
\(\alpha_i\) to \(f(\alpha_i) =\) \emph{binary representation of \(i-1\)
using exactly \(\ceil{\log n}\) bits}.
Like before, there is only one way to encode and decode the top and
bottom string of a match, hence there is a mapping from matches in the original
instance to matches of the constructed instance.

\subsection{Using a prefix-free encoding}
Indeed, any way of encoding the symbols of
\(\Gamma\) using \texttt{0} and \texttt{1} in such a way that a string of
\texttt{0} and \texttt{1} is uniquely decodable into a string over \(\Gamma\)
will work. A last example of such mappings is prefix-free encodings, that is,
mappings from \(\Gamma\) to \(\Sigma^*\) such that no encoding of a symbol of
\(\Gamma\) is a prefix of the encoding of another symbol. An example for
\(\Gamma = \{a,b,c\}\) is \(f(a) = \texttt{0}\), \(f(b) = \texttt{10}\), \(f(c)
= \texttt{11}\).

\section{Exercise 4.17\footnote{Exercise 4.16 in the second edition of the
reference book}}
A DFA is a \(5\)-tuple \((Q,\Sigma,\delta,q_0,F)\) where \(Q\) is the set of
states, \(\Sigma\) is the input alphabet, \(\delta : Q \times \Sigma \to Q\) is the transition
function, \(q_0 \in Q\) is the start state and \(F \subseteq Q\) is the set of
accept states. Without loss of generality, we consider the case \(\Sigma =
\{0,1\}\).
Given a DFA \(A\) and an input string \(w = w_1 w_2 \ldots w_n\) we can check
whether \(w \in L(A)\) by computing the value of the following function \(D_A\)
for \(w\)
\begin{align*}
	D_A(w) &= \alpha_A(D_A'(w))\\
	\alpha_A(q) &= \begin{cases}
		\accept & \text{if \(q \in F\)},\\
		\reject & \text{otherwise}
	\end{cases}\\
	D_A'(w) &= D_A''(q_0,w)\\
	D_A''(q,w_1 w_{2 \ldots n}) &= D_A''(\delta(q,w_1),w_{2 \ldots n})\\
	D_A''(q,\epsilon) &= q.
\end{align*}

Given two DFA's \(A_1 = (Q_1,\Sigma,\delta_1,q_0^1,F_1)\) and \(A_2 =
(Q_2,\Sigma,\delta_2,q_0^2,F_2)\) we want to check whether \(L(A_1) =
L(A_2)\). Imagine we are given an input \(w\) and run \(A_1\) and \(A_2\)
on their own copy of \(w\). Let \(e_1 = D_{A_1}'(w)\) be the last state reached by
\(A_1\) when run on \(w\) and let \(e_2 = D_{A_2}'(w)\) be the last state reached
by \(A_2\) when run on \(w\). Then \(L(A_1) = L(A_2) \implies D_{A_1}(w) =
D_{A_2}(w)\), that is, either both \(e_1\) and \(e_2\) are accepting, or
both are rejecting. Indeed, to show that \(L(A_1) = L(A_2)\) we only need to
show that each possible ending configuration \((e_1,e_2) \in (Q_1 \times Q_2)\) obtained by
simulating both \(A_1\) and \(A_2\) on an input word contain states that are
both accepting or rejecting. There are at most \(\card{Q_1} \card{Q_2}\) such
configurations. Here is an algorithm that checks all of them
\begin{TMachine}{On input \(\langle A_1, A_2 \rangle\):}
\item[1.] \(Z \gets \{\}\)
\item[2.] \(X \gets \{(q_0^1,q_0^2)\}\)
\item[3.] While \(X \neq \emptyset\):
\item[3.1.] Pick \((q_1,q_2) \in X\).
\item[3.2.] If \(\alpha_{A_1}(q_1) \neq \alpha_{A_2}(q_2)\), \reject.
\item[3.3.] \(Z \gets Z \cup \{(q_1,q_2)\}\)
\item[3.4.] \(X \gets X \cup
\{(\delta_1(q_1,0),\delta_2(q_2,0)),(\delta_1(q_1,1),\delta_2(q_2,1))\}
\setminus Z\)
\item[4.] \accept.
\end{TMachine}
Another equivalent (but less efficient) algorithm checks that \(A_1\) agrees
with \(A_2\) on all strings of length at most \(\card{Q_1} \card{Q_2}\).
% using the pigeon hole principle to prove this would be a better idea

\section{Exercises 3.15 and 3.16}
\subsection{Exercise 3.15}
\paragraph{(b)} \( A \circ B = \{ w_a w_b \st w_a \in A \land w_b \in B\}\),
that is, the words that are the result of concatenating a word of \(B\) to a
word of \(A\).

If we are given a \TM{} \(M_A\) that decides a language \(A\) and
a \TM{} \(M_B\) that decides a language \(B\) we can decide the
language \(A \circ B\) using \(M_{A \circ B}\) defined as follows
\begin{TMachine}{\(M_{A \circ B} =\) On input word \(w = w_1 w_2 \ldots w_n\):}
\item[1.] For each possible split \(i \in \{0,1,\ldots,n\}\)
\item[1.1.] Run \(M_A\) on \(w_1 \ldots w_i\). If it rejects, continue with next split.
\item[1.2.] Run \(M_B\) on \(w_{i+1} \ldots w_n\). If it accepts, \accept.
\item[2.] \reject.
\end{TMachine}

\paragraph{(d)}
\( \overline{A} = \Sigma^{*} \setminus A = \{ w \in \Sigma^{*} \st w \not \in A
	\}\), that is, words containing only symbols of the language alphabet that are not in the
	language.

If we are given a \TM{} \(M_A\) that decides a language \(A\)
we can decide the
language \(\overline{A}\) using \(M_{\overline{A}}\) defined as follows
\begin{TMachine}{\(M_{\overline{A}} =\) On input word \(w\):}
\item[1.] Run \(M_A\) on \(w\). If it rejects, \accept. Otherwise, \reject.
\end{TMachine}

\paragraph{(e)}
\( A \cap B = \{w \st w \in A \land w \in B\}\), that is, words that are in both \(A\) and \(B\).

If we are given a \TM{} \(M_A\) that decides a language \(A\) and
a \TM{} \(M_B\) that decides a language \(B\) we can decide the
language \(A \cap B\) using \(M_{A \cap B}\) defined as follows
\begin{TMachine}{\(M_{A \cap B} =\) On input word \(w\):}
\item[1.] Run \(M_A\) on \(w\). If it rejects, \reject.
\item[2.] Run \(M_B\) on \(w\). If it rejects, \reject. Otherwise, \accept.''
\end{TMachine}

\subsection{Exercise 3.16}
\paragraph{(b),(d)}
Like we already saw during the first exercise session, when dealing with
recognizers we must be careful not to trap ourselves in infinite loops. A
generic way for avoiding this trap is to run simulations in parallel. For
exercise \textbf{(b)} we do the simulations for all splits in
parallel. For exercise \textbf{(d)} we do not need this trick since both
machines must accept in order to accept. We can thus wait for the first to
accept before running the second, unlike exercise \textbf{(a)}.


\end{document}
